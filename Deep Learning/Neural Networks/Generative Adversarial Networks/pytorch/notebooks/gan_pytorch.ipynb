{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa894b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train(CONFIG)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a5ef4",
   "metadata": {},
   "source": [
    "### Interpret the losses\n",
    "- `d_loss` should oscillate while staying finite; collapse suggests discriminators dominating or vanishing gradients.\n",
    "- `g_loss` rising steadily indicates the generator struggling to fool the discriminator; tweak learning rates or betas.\n",
    "- Log the metrics JSON with experiment metadata for easier comparison across runs.\n",
    "- Plot the loss curves (`pandas.DataFrame(metrics).plot()`) to verify convergence trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generate_samples(CONFIG, num_images=16, output_path=CONFIG.artifact_dir / 'notebook_samples.png')\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b75d6c",
   "metadata": {},
   "source": [
    "### Next experiments\n",
    "- Try smaller batches to see how instability manifests and adjust gradient accumulation if needed.\n",
    "- Add label smoothing or noise to discriminator targets to improve training stability.\n",
    "- Replace transpose convolutions with upsampling + convolutions to reduce checkerboard artefacts.\n",
    "- Track Fr√©chet Inception Distance for objective quality measurements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
